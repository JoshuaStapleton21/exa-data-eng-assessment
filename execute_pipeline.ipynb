{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive pipeline runner notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import tools.read_data as rd\n",
    "import pandas as pd\n",
    "import unittest\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "for k,v in list(sys.modules.items()):\n",
    "    if k.startswith('tools'):\n",
    "        importlib.reload(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.251110315322876\n"
     ]
    }
   ],
   "source": [
    "# read in the json data using a generator method - usually slightly faster for larger datasets\n",
    "patient_json_list = []\n",
    "start = time.time()\n",
    "for json_obj in rd.read_json_files('data'):\n",
    "    patient_json_list.append(json_obj)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.689486026763916\n"
     ]
    }
   ],
   "source": [
    "# read in the json data using a standard method - usually slightly slower for larger datasets\n",
    "patient_json_list_alt = []\n",
    "start = time.time()\n",
    "patient_file_list = rd.get_file_list('data')\n",
    "for json_obj in patient_file_list:\n",
    "    patient_json_list_alt.append(rd.read_patient_file('data', json_obj))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have also implemented functions to read in the data from a database or an API - these are not implemented in this example\n",
    "# patient_json_list = rd.get_json_objects_from_API('https://www.example-patient-api.com/get-patient-FHIR-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the two different methods correctly calculated the same list\n",
    "assert patient_json_list == patient_json_list_alt, \"Two alternative methods of calculating the same FHIR data list have returned different results\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests for data quality / correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.372s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.206s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.108s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.338s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 5.321s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.314s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.205s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.144s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.276s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.327s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.050s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.170s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.333s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.521s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.188s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 3.995s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.126s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.277s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.124s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.268s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.213s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.379s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.087s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.231s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.100s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.176s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.037s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.296s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.144s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.126s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.376s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.092s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 1.400s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.411s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.161s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.220s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.144s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.096s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.187s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.494s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.086s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.417s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.204s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.313s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.094s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.192s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.096s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.280s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.041s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.107s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.313s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.367s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.194s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.210s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.091s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.111s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.524s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.226s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.316s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.265s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.196s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.289s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.291s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.090s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.176s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 1.859s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.133s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.294s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.440s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.744s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.281s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.052s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.331s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.153s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.058s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.176s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.617s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.401s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.312s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# run all tests on the incoming data\n",
    "from tools.data_tests import TestFHIRData\n",
    "\n",
    "test_runner = unittest.TextTestRunner()\n",
    "for json_obj in patient_json_list:\n",
    "    TestFHIRData.JSON_OBJ = json_obj\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestFHIRData)\n",
    "    test_results = test_runner.run(test_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FFFF\n",
      "======================================================================\n",
      "FAIL: test_all_fields_in_patient (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 43, in test_all_fields_in_patient\n",
      "    self.assertIn(field, expected_fields, msg=f\"{field} field not found in expected fields list\")\n",
      "AssertionError: 'subject' not found in ['resourceType', 'fhir_comments', 'id', 'implicitRules', 'implicitRules__ext', 'language', 'language__ext', 'meta', 'contained', 'extension', 'modifierExtension', 'text', 'active', 'active__ext', 'address', 'birthDate', 'birthDate__ext', 'communication', 'contact', 'deceasedBoolean', 'deceasedBoolean__ext', 'deceasedDateTime', 'deceasedDateTime__ext', 'gender', 'gender__ext', 'generalPractitioner', 'identifier', 'link', 'managingOrganization', 'maritalStatus', 'multipleBirthBoolean', 'multipleBirthBoolean__ext', 'multipleBirthInteger', 'multipleBirthInteger__ext', 'name', 'photo', 'telecom'] : subject field not found in expected fields list\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_json_obj_is_fhir_bundle (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 20, in test_json_obj_is_fhir_bundle\n",
      "    bundle = Bundle.parse_obj(self.JSON_OBJ)\n",
      "pydantic.error_wrappers.ValidationError: 5 validation errors for Bundle\n",
      "entry -> 0 -> resource -> __root__ -> status\n",
      "  field required (type=value_error.missing)\n",
      "entry -> 1 -> resource -> name -> 0 -> given\n",
      "  value is not a valid list (type=type_error.list)\n",
      "entry -> 1 -> resource -> socialSecurityNumber\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "entry -> 2 -> resource -> name -> 0 -> given\n",
      "  value is not a valid list (type=type_error.list)\n",
      "entry -> 3 -> resource -> __root__ -> status\n",
      "  field required (type=value_error.missing)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 22, in test_json_obj_is_fhir_bundle\n",
      "    self.fail(\"JSON object is not a FHIR Bundle\")\n",
      "AssertionError: JSON object is not a FHIR Bundle\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_only_one_patient_field_per_bundle (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 34, in test_only_one_patient_field_per_bundle\n",
      "    self.assertEqual(patient_count, 1, msg=\"Multiple patient fields found in bundle\")\n",
      "AssertionError: 2 != 1 : Multiple patient fields found in bundle\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_patient_is_first_entry_in_list (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 15, in test_patient_is_first_entry_in_list\n",
      "    self.assertEqual(self.JSON_OBJ['entry'][0]['resource']['resourceType'], \"Patient\", msg=\"Patient object not first in entry list\")\n",
      "AssertionError: 'Observation' != 'Patient'\n",
      "- Observation\n",
      "+ Patient\n",
      " : Patient object not first in entry list\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.024s\n",
      "\n",
      "FAILED (failures=4)\n"
     ]
    }
   ],
   "source": [
    "# example test failure\n",
    "with open(\"resources/bad_example.json\") as f:\n",
    "    bad_json_obj = json.load(f)\n",
    "\n",
    "TestFHIRData.JSON_OBJ = bad_json_obj\n",
    "test_suite = unittest.TestLoader().loadTestsFromTestCase(TestFHIRData)\n",
    "test_results = test_runner.run(test_suite)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic / local implementation of pipeline using filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhir.resources.patient import Patient\n",
    "from fhir.resources.bundle import Bundle\n",
    "\n",
    "FHIR_patient_object_list = [Patient.parse_obj(Bundle.parse_obj(patient_json).entry[0].resource) for patient_json in patient_json_list]\n",
    "patient_df = rd.patients_to_dataframe(FHIR_patient_object_list).drop(columns=['resource_type']) # we can drop this column because it is constant by definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the raw tabular data to a csv file. This needs to be normalized and cleaned before it can be used for analysis.\n",
    "patient_df.to_csv('data_output/patient_data_tabular_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding column: address\n",
      "exploding column: communication\n",
      "exploding column: identifier\n",
      "exploding column: name\n",
      "exploding column: telecom\n"
     ]
    }
   ],
   "source": [
    "# 1NF normalization - each table cell should have a single value\n",
    "# the columns in the dataframe in need of normalization are extension, address, maritalStatus, name, telecom, etc.\n",
    "# a naive solution would be to explode the columns that are lists. This, however, tends to become monolithic, as the number of table rows grows exponentially.\n",
    "print(\"exploding column: extension\")\n",
    "patient_exploded_df = patient_df.explode('extension') # start by exploding extension - the first column of type list\n",
    "for column in patient_df.columns.drop('extension'):\n",
    "    if type(patient_df[column][0]) == list:\n",
    "        print(\"exploding column: \" + column)\n",
    "        patient_exploded_df = patient_exploded_df.explode(column)\n",
    "\n",
    "patient_exploded_df.to_csv('data_output/1NF_data/patient_data_tabular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding column: extension\n",
      "exploding column: address\n",
      "exploding column: communication\n",
      "exploding column: identifier\n",
      "exploding column: name\n",
      "exploding column: telecom\n"
     ]
    }
   ],
   "source": [
    "# 2NF normalization - create additional tables for initial table cells with multiple/list entires\n",
    "# this is a more complex solution, but it is more scalable, easier to maintain, and there is less data redundancy\n",
    "patient_df_2NF = patient_df.copy()\n",
    "\n",
    "for column in patient_df_2NF.columns:\n",
    "    if type(patient_df_2NF[column][0]) == list:\n",
    "        print(\"exploding column: \" + column)\n",
    "        patient_exploded_df = patient_df_2NF.explode(column)\n",
    "        patient_df_2NF = patient_df_2NF.drop(columns=[column])\n",
    "\n",
    "        # drop all columns from the exploded dataframe that are in the original dataframe except ID\n",
    "        NF_columns = list(patient_df_2NF.columns)\n",
    "        NF_columns.remove('id')\n",
    "        patient_exploded_df.drop(columns=NF_columns, inplace=True)\n",
    "        patient_exploded_df.to_csv('data_output/2NF_data/patient_data_tabular_' + column + '.csv', index=False)\n",
    "\n",
    "# finally, write the original table with all multi-value columns removed to a csv file\n",
    "patient_df_2NF.to_csv('data_output/2NF_data/patient_data_tabular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  we can further expand the table by identifying values which are FHIR objects and splitting them up by field, however this can get tricky with string parsing. \n",
    "# For now, we will write the whole objects encoded as strings.\n",
    "\n",
    "# patient_1NF_df = pd.read_csv('data_output/1NF_data/patient_data_tabular.csv')\n",
    "# for column in patient_1NF_df.columns:\n",
    "#     # if the column starts with 'resource'\n",
    "#     first_column_value = patient_1NF_df[column].values[0]\n",
    "#     if type(first_column_value) == str and first_column_value.startswith('resource_type'):\n",
    "#         print(\"Fields of column:\", column)\n",
    "#         for field in first_column_value.split(' '):\n",
    "#             print(\"--\",field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL implementation of pipeline with database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import pandas as pd\n",
    "import tools.read_data as rd\n",
    "import tools.create_database as cd\n",
    "import tools.update_database as ud\n",
    "from sqlalchemy import text\n",
    "import importlib\n",
    "for k,v in list(sys.modules.items()):\n",
    "    if k.startswith('tools'):\n",
    "        importlib.reload(v)\n",
    "\n",
    "# get/open the connection to the patient database\n",
    "patient_database_conn = cd.connect_to_sqla_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.CursorResult at 0x1077e2a00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping tables if they already exist\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS patient\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS address\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS communication\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS extension\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS identifier\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS name\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS telecom\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tables in the patient database with 2NF standards - these correspond to the csv files in the data_output 2NF folder generated in the previous steps\n",
    "TWONF_CREATE_PATIENT_TABLE_SQL = \"\"\"\n",
    "        CREATE TABLE patient (\n",
    "            fhir_comments string,\n",
    "            id string,\n",
    "            implicitRules string,\n",
    "            implicitRules__ext string,\n",
    "            language string,\n",
    "            language__ext string,\n",
    "            meta string,\n",
    "            contained string,\n",
    "            modifierExtension string,\n",
    "            text string,\n",
    "            active string,\n",
    "            active__ext string,\n",
    "            birthDate string,\n",
    "            birthDate__ext string,\n",
    "            contact string,\n",
    "            deceasedBoolean string,\n",
    "            deceasedBoolean__ext string,\n",
    "            deceasedDateTime string,\n",
    "            deceasedDateTime__ext string,\n",
    "            gender string,\n",
    "            gender__ext string,\n",
    "            generalPractitioner string,\n",
    "            link string,\n",
    "            managingOrganization string,\n",
    "            maritalStatus string,\n",
    "            multipleBirthBoolean string,\n",
    "            multipleBirthBoolean__ext string,\n",
    "            multipleBirthInteger string,\n",
    "            multipleBirthInteger__ext string,\n",
    "            photo string\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "ud.execute_sql(TWONF_CREATE_PATIENT_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the address table\n",
    "TWONF_CREATE_ADDRESS_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE address (\n",
    "        id bool,\n",
    "        address string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_ADDRESS_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the communication table\n",
    "TWONF_CREATE_COMMUNICATION_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE communication (\n",
    "        id string,\n",
    "        communication string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_COMMUNICATION_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the extension table\n",
    "TWONF_CREATE_EXTENSION_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE extension (\n",
    "        id string,\n",
    "        extension string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_EXTENSION_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the identifier table\n",
    "TWONF_CREATE_IDENTIFIER_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE identifier (\n",
    "        id string,\n",
    "        identifier string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_IDENTIFIER_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the name table\n",
    "TWONF_CREATE_NAME_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE name (\n",
    "        id string,\n",
    "        name string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_NAME_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the telecom table\n",
    "TWONF_CREATE_TELECOM_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE telecom (\n",
    "        id string, \n",
    "        telecom string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_TELECOM_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# IMPORTANT:\n",
    "# I decided to use the generic, more pythonic variable types for the database table fields (ie: string as opposed to VARCHAR).\n",
    "# This is because I am not 100% sure what types of data will be coming in, and if FHIR data types map exactly to SQL data types.\n",
    "# Once we have a stronger idea of the data types based on incoming data consistency, we can update the table fields to be more specific and SQL compliant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, populate the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the main table.\n",
    "# We can workaround writing any physical artifacts by simply writing the dataframes as calculated above to the database.\n",
    "patient_df = pd.read_csv('data_output/2NF_data/patient_data_tabular.csv')\n",
    "\n",
    "def replace_quotes(df): # this can mess up formatting\n",
    "    \"\"\"\n",
    "    Replaces all instances of \" with ' in all cells in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return df.applymap(lambda x: str(x).replace(\"\\\"\", \"'\"))\n",
    "patient_df = replace_quotes(patient_df)\n",
    "\n",
    "for index, row in patient_df.iterrows():\n",
    "    values = row.to_list()\n",
    "    values_string = '\\\", \\\"'.join(values)\n",
    "    values_string = '\\\"'+values_string+'\\\"'\n",
    "    insert_sql = \"INSERT INTO patient (fhir_comments, id, implicitRules, implicitRules__ext, language, language__ext, meta, contained, modifierExtension, text, active, active__ext, birthDate, birthDate__ext, contact, deceasedBoolean, deceasedBoolean__ext, deceasedDateTime, deceasedDateTime__ext, gender, gender__ext, generalPractitioner, link, managingOrganization, maritalStatus, multipleBirthBoolean, multipleBirthBoolean__ext, multipleBirthInteger, multipleBirthInteger__ext, photo) VALUES (\"+values_string+\")\"\n",
    "    patient_database_conn.execute(text(insert_sql))\n",
    "\n",
    "# populate the sub tables\n",
    "sub_table_list = rd.get_file_list('data_output/2NF_data')\n",
    "for table in sub_table_list[:-1]: # populate all sub tables\n",
    "    table_data = pd.read_csv('data_output/2NF_data/'+table)\n",
    "    table_name = str(table_data.columns[1]) # same as the second column name\n",
    "    for index, row in table_data.iterrows():\n",
    "        patient_id = \"\\\"\"+row['id']+\"\\\"\"\n",
    "        # enclose with double quotes to ensure it is recognized as a string\n",
    "        value = \"\\\"\"+row[table_name].replace(\"\\\"\",\"'\")+\"\\\"\"\n",
    "        insert_sql = \"INSERT INTO \"+table_name+\" (id, \"+table_name+\") VALUES ({}, {})\".format(patient_id, value)\n",
    "        patient_database_conn.execute(text(insert_sql))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='Extension' fhir_comments=None e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "1    8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "2    8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "3    8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "4    8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "..                                    ...   \n",
       "548  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "549  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "550  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "551  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "552  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "\n",
       "                                             extension  \n",
       "0    resource_type='Extension' fhir_comments=None e...  \n",
       "1    resource_type='Extension' fhir_comments=None e...  \n",
       "2    resource_type='Extension' fhir_comments=None e...  \n",
       "3    resource_type='Extension' fhir_comments=None e...  \n",
       "4    resource_type='Extension' fhir_comments=None e...  \n",
       "..                                                 ...  \n",
       "548  resource_type='Extension' fhir_comments=None e...  \n",
       "549  resource_type='Extension' fhir_comments=None e...  \n",
       "550  resource_type='Extension' fhir_comments=None e...  \n",
       "551  resource_type='Extension' fhir_comments=None e...  \n",
       "552  resource_type='Extension' fhir_comments=None e...  \n",
       "\n",
       "[553 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how to query the database\n",
    "result = patient_database_conn.execute(text(\"SELECT * FROM extension\"))\n",
    "df = pd.DataFrame(result.all(), columns=result.keys())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
