{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import tools.read_data as rd\n",
    "import pandas as pd\n",
    "import unittest\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "for k,v in list(sys.modules.items()):\n",
    "    if k.startswith('tools'):\n",
    "        importlib.reload(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not read file - please pass a different directory or filename\n",
      "1.468338966369629\n"
     ]
    }
   ],
   "source": [
    "# read in the json data using a generator method\n",
    "patient_json_list = []\n",
    "start = time.time()\n",
    "for json_obj in rd.read_json_files('data'):\n",
    "    patient_json_list.append(json_obj)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.676032781600952\n"
     ]
    }
   ],
   "source": [
    "# read in the json data using a standard method\n",
    "patient_json_list_alt = []\n",
    "start = time.time()\n",
    "pfl = rd.get_patient_file_list('data')\n",
    "for json_obj in pfl:\n",
    "    patient_json_list_alt.append(rd.read_patient_file('data', json_obj))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have also implemented functions to read in the data from a database or an API - these are not implemented in this example\n",
    "# patient_json_list = rd.get_json_objects_from_API('https://www.example-patient-api.com/get-patient-FHIR-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the two different methods correctly calculated the same list\n",
    "assert patient_json_list == patient_json_list_alt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests for data quality / correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.346s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.262s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.150s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.485s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 11.012s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.420s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.327s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.216s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.494s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.556s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.073s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.244s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.438s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 5.154s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.269s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.106s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.150s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.365s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.182s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.365s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.296s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.516s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.112s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.324s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.133s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.240s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.055s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.406s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.205s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.174s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.509s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.125s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.304s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.387s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.212s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.286s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.191s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.129s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.244s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.789s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.118s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.541s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.228s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.389s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.115s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.248s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.120s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.312s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.052s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.133s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.128s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.400s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.219s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.278s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.112s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.102s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.548s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.236s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.396s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.287s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.253s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.363s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.214s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.120s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.223s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.471s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.174s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.359s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.600s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.961s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.363s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.072s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.415s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.187s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.080s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.240s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 1.409s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.876s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.419s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# run all tests on the incoming data\n",
    "from tools.data_tests import TestFHIRData\n",
    "\n",
    "test_runner = unittest.TextTestRunner()\n",
    "for json_obj in patient_json_list:\n",
    "    TestFHIRData.JSON_OBJ = json_obj\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestFHIRData)\n",
    "    test_results = test_runner.run(test_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FFFF\n",
      "======================================================================\n",
      "FAIL: test_all_fields_in_patient (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 43, in test_all_fields_in_patient\n",
      "    self.assertIn(field, expected_fields, msg=f\"{field} field not found in expected fields list\")\n",
      "AssertionError: 'subject' not found in ['resourceType', 'fhir_comments', 'id', 'implicitRules', 'implicitRules__ext', 'language', 'language__ext', 'meta', 'contained', 'extension', 'modifierExtension', 'text', 'active', 'active__ext', 'address', 'birthDate', 'birthDate__ext', 'communication', 'contact', 'deceasedBoolean', 'deceasedBoolean__ext', 'deceasedDateTime', 'deceasedDateTime__ext', 'gender', 'gender__ext', 'generalPractitioner', 'identifier', 'link', 'managingOrganization', 'maritalStatus', 'multipleBirthBoolean', 'multipleBirthBoolean__ext', 'multipleBirthInteger', 'multipleBirthInteger__ext', 'name', 'photo', 'telecom'] : subject field not found in expected fields list\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_json_obj_is_fhir_bundle (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 20, in test_json_obj_is_fhir_bundle\n",
      "    bundle = Bundle.parse_obj(self.JSON_OBJ)\n",
      "pydantic.error_wrappers.ValidationError: 5 validation errors for Bundle\n",
      "entry -> 0 -> resource -> __root__ -> status\n",
      "  field required (type=value_error.missing)\n",
      "entry -> 1 -> resource -> name -> 0 -> given\n",
      "  value is not a valid list (type=type_error.list)\n",
      "entry -> 1 -> resource -> socialSecurityNumber\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "entry -> 2 -> resource -> name -> 0 -> given\n",
      "  value is not a valid list (type=type_error.list)\n",
      "entry -> 3 -> resource -> __root__ -> status\n",
      "  field required (type=value_error.missing)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 22, in test_json_obj_is_fhir_bundle\n",
      "    self.fail(\"JSON object is not a FHIR Bundle\")\n",
      "AssertionError: JSON object is not a FHIR Bundle\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_only_one_patient_field_per_bundle (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 34, in test_only_one_patient_field_per_bundle\n",
      "    self.assertEqual(patient_count, 1, msg=\"Multiple patient fields found in bundle\")\n",
      "AssertionError: 2 != 1 : Multiple patient fields found in bundle\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_patient_is_first_entry_in_list (tools.data_tests.TestFHIRData)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/joshuastapleton/Desktop/EMIS_interview/exa-data-eng-assessment/tools/data_tests.py\", line 15, in test_patient_is_first_entry_in_list\n",
      "    self.assertEqual(self.JSON_OBJ['entry'][0]['resource']['resourceType'], \"Patient\", msg=\"Patient object not first in entry list\")\n",
      "AssertionError: 'Observation' != 'Patient'\n",
      "- Observation\n",
      "+ Patient\n",
      " : Patient object not first in entry list\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.024s\n",
      "\n",
      "FAILED (failures=4)\n"
     ]
    }
   ],
   "source": [
    "# example test failure\n",
    "with open(\"resources/bad_example.json\") as f:\n",
    "    bad_json_obj = json.load(f)\n",
    "\n",
    "TestFHIRData.JSON_OBJ = bad_json_obj\n",
    "test_suite = unittest.TestLoader().loadTestsFromTestCase(TestFHIRData)\n",
    "test_results = test_runner.run(test_suite)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic / local implementation of pipeline using filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhir.resources.patient import Patient\n",
    "from fhir.resources.bundle import Bundle\n",
    "\n",
    "FHIR_patient_object_list = [Patient.parse_obj(Bundle.parse_obj(patient_json).entry[0].resource) for patient_json in patient_json_list]\n",
    "patient_df = rd.patients_to_dataframe(FHIR_patient_object_list).drop(columns=['resource_type']) # we can drop this column because it is constant by definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the raw tabular data to a csv file. This needs to be normalized and cleaned before it can be used for analysis.\n",
    "patient_df.to_csv('data_output/patient_data_tabular_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding column: address\n",
      "exploding column: communication\n",
      "exploding column: identifier\n",
      "exploding column: name\n",
      "exploding column: telecom\n"
     ]
    }
   ],
   "source": [
    "# 1NF normalization - each table cell should have a single value\n",
    "# the columns in the dataframe in need of normalization are extension, address, maritalStatus, name, telecom, etc.\n",
    "# a naive solution would be to explode the columns that are lists. This, however, tends to become monolithic, as the number of table rows grows exponentially.\n",
    "print(\"exploding column: extension\")\n",
    "patient_exploded_df = patient_df.explode('extension') # start by exploding extension - the first column of type list\n",
    "for column in patient_df.columns.drop('extension'):\n",
    "    if type(patient_df[column][0]) == list:\n",
    "        print(\"exploding column: \" + column)\n",
    "        patient_exploded_df = patient_exploded_df.explode(column)\n",
    "\n",
    "patient_exploded_df.to_csv('data_output/1NF_data/patient_data_tabular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding column: extension\n",
      "exploding column: address\n",
      "exploding column: communication\n",
      "exploding column: identifier\n",
      "exploding column: name\n",
      "exploding column: telecom\n"
     ]
    }
   ],
   "source": [
    "# 2NF normalization - create additional tables for initial table cells with multiple/list entires\n",
    "# this is a more complex solution, but it is more scalable, easier to maintain, and there is less data redundancy\n",
    "patient_df_2NF = patient_df.copy()\n",
    "\n",
    "for column in patient_df_2NF.columns:\n",
    "    if type(patient_df_2NF[column][0]) == list:\n",
    "        print(\"exploding column: \" + column)\n",
    "        patient_exploded_df = patient_df_2NF.explode(column)\n",
    "        patient_df_2NF = patient_df_2NF.drop(columns=[column])\n",
    "\n",
    "        # drop all columns from the exploded dataframe that are in the original dataframe except ID\n",
    "        NF_columns = list(patient_df_2NF.columns)\n",
    "        NF_columns.remove('id')\n",
    "        patient_exploded_df.drop(columns=NF_columns, inplace=True)\n",
    "        patient_exploded_df.to_csv('data_output/2NF_data/patient_data_tabular_' + column + '.csv', index=False)\n",
    "\n",
    "# finally, write the original table with all multi-value columns removed to a csv file\n",
    "patient_df_2NF.to_csv('data_output/2NF_data/patient_data_tabular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  we can further expand the table by identifying values which are FHIR objects and splitting them up by field, however this can get tricky with string parsing\n",
    "# patient_1NF_df = pd.read_csv('data_output/1NF_data/patient_data_tabular.csv')\n",
    "# for column in patient_1NF_df.columns:\n",
    "#     # if the column starts with 'resource'\n",
    "#     first_column_value = patient_1NF_df[column].values[0]\n",
    "#     if type(first_column_value) == str and first_column_value.startswith('resource_type'):\n",
    "#         print(\"Fields of column:\", column)\n",
    "#         for field in first_column_value.split(' '):\n",
    "#             print(\"--\",field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL implementation of pipeline with database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import pandas as pd\n",
    "import tools.create_database as cd\n",
    "import tools.update_database as ud\n",
    "from sqlalchemy import text\n",
    "import importlib\n",
    "for k,v in list(sys.modules.items()):\n",
    "    if k.startswith('tools'):\n",
    "        importlib.reload(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.CursorResult at 0x1032fb580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get/open the connection to the patient database, dropping contents if it already exists\n",
    "patient_database_conn = cd.connect_to_sqla_server()\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS patient\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS address\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS communication\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS extension\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS identifier\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS name\"))\n",
    "patient_database_conn.execute(text(\"DROP TABLE IF EXISTS telecom\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tables in the patient database with 2NF standards - these correspond to the csv files in the data_output 2NF folder generated in the previous steps\n",
    "TWONF_CREATE_PATIENT_TABLE_SQL = \"\"\"\n",
    "        CREATE TABLE patient (\n",
    "            fhir_comments VARCHAR(255),\n",
    "            id VARCHAR(36),\n",
    "            implicitRules VARCHAR(255),\n",
    "            implicitRules__ext VARCHAR(255),\n",
    "            language VARCHAR(255),\n",
    "            language__ext VARCHAR(255),\n",
    "            meta VARCHAR(255),\n",
    "            contained VARCHAR(255),\n",
    "            modifierExtension VARCHAR(255),\n",
    "            text VARCHAR(255),\n",
    "            active BOOLEAN,\n",
    "            active__ext VARCHAR(255),\n",
    "            birthDate DATE,\n",
    "            birthDate__ext VARCHAR(255),\n",
    "            contact VARCHAR(255),\n",
    "            deceasedBoolean BOOLEAN,\n",
    "            deceasedBoolean__ext VARCHAR(255),\n",
    "            deceasedDateTime DATE,\n",
    "            deceasedDateTime__ext VARCHAR(255),\n",
    "            gender VARCHAR(255),\n",
    "            gender__ext VARCHAR(255),\n",
    "            generalPractitioner VARCHAR(255),\n",
    "            link VARCHAR(255),\n",
    "            managingOrganization VARCHAR(255),\n",
    "            maritalStatus VARCHAR(255),\n",
    "            multipleBirthBoolean BOOLEAN,\n",
    "            multipleBirthBoolean__ext bool,\n",
    "            multipleBirthInteger int,\n",
    "            multipleBirthInteger__ext VARCHAR(255),\n",
    "            photo string\n",
    "        );\n",
    "    \"\"\"\n",
    "\n",
    "# ud.create_table(TWONF_CREATE_PATIENT_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the address table\n",
    "TWONF_CREATE_ADDRESS_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE address (\n",
    "        id bool,\n",
    "        address string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_ADDRESS_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the communication table\n",
    "TWONF_CREATE_COMMUNICATION_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE communication (\n",
    "        id string,\n",
    "        communication string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_COMMUNICATION_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the extension table\n",
    "TWONF_CREATE_EXTENSION_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE extension (\n",
    "        id string,\n",
    "        extension string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_EXTENSION_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the identifier table\n",
    "TWONF_CREATE_IDENTIFIER_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE identifier (\n",
    "        id string,\n",
    "        identifier string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_IDENTIFIER_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the name table\n",
    "TWONF_CREATE_NAME_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE name (\n",
    "        id string,\n",
    "        name string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_NAME_TABLE_SQL, patient_database_conn)\n",
    "\n",
    "# create the telecom table\n",
    "TWONF_CREATE_TELECOM_TABLE_SQL = \"\"\"\n",
    "    CREATE TABLE telecom (\n",
    "        id string, \n",
    "        telecom string\n",
    "    );\n",
    "\"\"\"\n",
    "ud.execute_sql(TWONF_CREATE_TELECOM_TABLE_SQL, patient_database_conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, populate the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_data = pd.read_csv('data_output/2NF_data/patient_data_tabular_telecom.csv')\n",
    "\n",
    "for index, row in telecom_data.iterrows():\n",
    "    patient_id = \"\\\"\"+row['id']+\"\\\"\"\n",
    "    # enclose with double quotes\n",
    "    address = \"\\\"\"+row['telecom']+\"\\\"\"\n",
    "    insert_telecom_sql = \"INSERT INTO telecom (id, telecom) VALUES ({}, {})\".format(patient_id, address)\n",
    "    patient_database_conn.execute(text(insert_telecom_sql))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>telecom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a3247d3-a54c-43f2-2c5d-a8f5e28ff588</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0f40536-9dc8-2ea0-0bbf-467a69f5e3ad</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09e292d4-f186-331c-ed95-c503acabc54e</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10bf6da8-ffa1-6913-a119-726634be754c</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d55a582-07fe-a897-776c-3ab5e48cd457</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>f406a4e8-821b-0c9a-c8ec-09ad0f1fe9c6</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>aad9d04b-bb30-2f47-d5dd-888b3b7bd831</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>aa4eae2c-733a-35f9-8869-d33a6015db23</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cf3ce382-cceb-1557-89ac-b751a9e0e65d</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>b0bccf43-3bf5-217c-7315-9e44d106bb6b</td>\n",
       "      <td>resource_type='ContactPoint' fhir_comments=Non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "0   8a3247d3-a54c-43f2-2c5d-a8f5e28ff588   \n",
       "1   b0f40536-9dc8-2ea0-0bbf-467a69f5e3ad   \n",
       "2   09e292d4-f186-331c-ed95-c503acabc54e   \n",
       "3   10bf6da8-ffa1-6913-a119-726634be754c   \n",
       "4   0d55a582-07fe-a897-776c-3ab5e48cd457   \n",
       "..                                   ...   \n",
       "74  f406a4e8-821b-0c9a-c8ec-09ad0f1fe9c6   \n",
       "75  aad9d04b-bb30-2f47-d5dd-888b3b7bd831   \n",
       "76  aa4eae2c-733a-35f9-8869-d33a6015db23   \n",
       "77  cf3ce382-cceb-1557-89ac-b751a9e0e65d   \n",
       "78  b0bccf43-3bf5-217c-7315-9e44d106bb6b   \n",
       "\n",
       "                                              telecom  \n",
       "0   resource_type='ContactPoint' fhir_comments=Non...  \n",
       "1   resource_type='ContactPoint' fhir_comments=Non...  \n",
       "2   resource_type='ContactPoint' fhir_comments=Non...  \n",
       "3   resource_type='ContactPoint' fhir_comments=Non...  \n",
       "4   resource_type='ContactPoint' fhir_comments=Non...  \n",
       "..                                                ...  \n",
       "74  resource_type='ContactPoint' fhir_comments=Non...  \n",
       "75  resource_type='ContactPoint' fhir_comments=Non...  \n",
       "76  resource_type='ContactPoint' fhir_comments=Non...  \n",
       "77  resource_type='ContactPoint' fhir_comments=Non...  \n",
       "78  resource_type='ContactPoint' fhir_comments=Non...  \n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = patient_database_conn.execute(text(\"SELECT * FROM telecom\"))\n",
    "df = pd.DataFrame(result.all(), columns=result.keys())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
